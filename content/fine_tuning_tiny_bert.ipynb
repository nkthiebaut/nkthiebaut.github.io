{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "toxic_comments_transformer_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USc1-SUfsxg5"
      },
      "source": [
        "Over the past few years, transformers have spurted tremendous progress in natural language processing. This success is partly explained by their impressive scaling capabilities. It seems as if their perfomance improvement with the number of parameters had no ceiling, as emphasized in the [GPT-3 paper](https://arxiv.org/abs/2005.14165) by OpenAI. Following this observation, transformers models have become bigger and bigger. The latest [Switch Transformer](https://venturebeat.com/2021/01/12/google-trained-a-trillion-parameter-ai-language-model/) model from Google has more than a trillion parameters. \n",
        "\n",
        "Using those humongous models for practical applications is challenging. Fine-tuning a transformer model for custom use-cases requires special hardware and setups, is costly, time-consuming, and inference may be slow. \n",
        "\n",
        "## Knowledge distillation to the rescue\n",
        "\n",
        "<img src=\"https://github.com/nkthiebaut/nkthiebaut.github.io/blob/master/images/knowledge_distillation.png?raw=true\" alt=\"drawing\" width=\"75%\"/>\n",
        "\n",
        "Luckily, part of the ML community focuses on producing leaner and faster transformers while retaining most of their predictive power. This is usually done via knowledge distillation, where a small model (the \"student\") is trained to reproduce the predictions of a larger model (the \"teacher\"). Knowledge distillation is different from the original classification task since for a given example the student's predictions ($s_i$) have to match probabilities predicted by the teacher ($t_i$), rather than binary labels. To train the student, one typically combines the regular log loss (that doesn't depend on the teacher) $L_\\text{classif.} = \\sum_i y_i \\log (s_i)$ with a *distillation loss* (that takes into account the teacher) $L_\\text{distil.} = \\sum_i t_i \\log (s_i).$\n",
        "\n",
        "In practice, knowledge distillation benefits from a bunch of tricks such as probabilities smoothing, adding other terms to the loss functions, or pre-training the student modeel. More details [here](https://en.wikipedia.org/wiki/Knowledge_distillation).\n",
        "\n",
        "\n",
        "\n",
        "## Using smaller transformers without missing out on performance\n",
        "\n",
        "In my last post I've shown how to fine-tune [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf) for a multi-label classification task. DistilBERT is already a distilled version of BERT-base, with ~60M parameters instead of ~110M, but in this post my goal is to evaluate much smaller distilled versions of BERT, down to BERT-Tiny that was introduced in [a Google paper](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2) with a slew of distilled BERT models. BERT-Tiny has only 2 transformer layers with 128 hidden units each (distributed among 2 self-attention heads), totalling a lean 4.4M parameters.\n",
        "\n",
        "In addition to evaluating how fast and accurate this small transformer can be, I also introduce a few more items from the transformers toolbox. Notably, I use the PyTorch interface of the Transformers package. Like a lot of pre-trained transformers available in the Transformers package, the distilled versions of BERT mentionned above are only available as PyTorch models. \n",
        "\n",
        "I run all of the experiments on the Toxic Comments multi-label classification dataset, for comparison with my previous post where I trained a \"TD-IDF + logistic regression\" baseline (validation loss = 0.281) and fine-tuned a DistillBERT model (validation loss = 0.043)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9gdQ-iucHIL",
        "outputId": "f8ce17a9-0149-4be7-bf71-c4d444b71bac"
      },
      "source": [
        "%%capture\n",
        "!pip uninstall -y kaggle && pip install kaggle \n",
        "# Needed to get the latest version of the Kaggle CLI\n",
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# We'll use the Kaggle-CLI to download the dataset\n",
        "# To create an authentication token on Kaggle check https://www.kaggle.com/docs/api\n",
        "# You'll also have to accept the competition rules here: \n",
        "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/rules\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = getpass(prompt='Kaggle username: ')\n",
        "os.environ[\"KAGGLE_KEY\"] = getpass(prompt='Token: ')\n",
        "\n",
        "!kaggle --version\n",
        "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
        "!unzip jigsaw-toxic-comment-classification-challenge.zip && unzip train.csv.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kaggle username: ··········\n",
            "Token: ··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keMJm46HgZdg",
        "outputId": "8334a029-a2a5-49f7-9e4c-14c0f66443a7"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "texts = list(dataset[\"comment_text\"])\n",
        "label_names = dataset.drop([\"id\", \"comment_text\"], axis=1).columns\n",
        "labels = dataset[label_names].values\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "sample_idx = 23\n",
        "print(f'Sample: \"{train_texts[sample_idx]}\"')\n",
        "print(f\"Labels: {pd.Series(train_labels[sample_idx], label_names).to_dict()}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample: \"what the fuck who deleted the spider loc and hot rod sections fucking wikipedia stupid ass ignorant people can we get it back ?\"\n",
            "Labels: {'toxic': 1, 'severe_toxic': 0, 'obscene': 1, 'threat': 0, 'insult': 1, 'identity_hate': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjnTj8zffxCy"
      },
      "source": [
        "## Fine-tuning BERT-Tiny\n",
        "\n",
        "The BERT-Tiny model is available with 24 distilled models under the [Google transformers repository](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2), accessible through the [Transformers package](https://huggingface.co/transformers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SZPW0EZZ1UX",
        "outputId": "8f4f005a-139b-42ae-ec0a-df3c741927a0"
      },
      "source": [
        "!pip install -q transformers > /dev/null\n",
        "\n",
        "import transformers\n",
        "print(f\"Transformers package version: {transformers.__version__}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformers package version: 4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ7qHyVcSGQD"
      },
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
        "transformer_model = AutoModel.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ0jS58zgPZz"
      },
      "source": [
        "We've now loaded th pre-trained models. Let's create a PyTorch Dataset as an interface between the data used for fine-tuning the model, and the model. \n",
        "\n",
        "The Dataset interface is convenient because it allows you to handle datasets that fit entirely in the memory (as is the case for us now), but also datasets that are larger and have to be pre-loaded by chunks from the disk during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm7uq4hAUGXN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformersDataset(Dataset):\n",
        "    \"\"\"Encode texts and return them with their corresponding labels.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        \"\"\"Tokenize the corpus of texts and store them with the labels,\n",
        "        as PyTorch tensors.\"\"\"\n",
        "        self.encoded_inputs = tokenizer(texts, truncation=True, padding=True, \n",
        "            max_length=max_length, return_tensors=\"pt\")\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"In PyTorch datasets have to override the length method.\"\"\"\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index: int) -> dict:\n",
        "        \"\"\"This method defines how to feed the data during model training.\"\"\"\n",
        "        inputs_ands_labels = dict()\n",
        "        for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
        "            inputs_ands_labels[key] = self.encoded_inputs[key][index]\n",
        "        inputs_ands_labels['labels'] = self.labels[index]\n",
        "        return inputs_ands_labels\n",
        "\n",
        "train_dataset = TransformersDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
        "test_dataset = TransformersDataset(test_texts, test_labels, tokenizer, MAX_LENGTH)\n",
        "\n",
        "training_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "testing_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o9Loaqghpgj"
      },
      "source": [
        "We can now define the PyTorch model. Note that we have to define a custom model because we are dealing with a custom use-case (multi-label classification). For multi-class classification you can directly fine-tune the transformer model without extracting it into a PyTorch module.\n",
        "\n",
        "The signature of our custom transformer is chosen to be compatible with the [Transformers Trainer](https://huggingface.co/transformers/main_classes/trainer.html), a very convenient class that does all of the training and logging for us. If you want to see the full PyTorch version that doesn't use the Trainer, but relies on [PyTorch Lightning](https://www.pytorchlightning.ai/) instead, check the bonus section at the end of this post. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgUUp2eFUzIr"
      },
      "source": [
        "from torch.nn.functional import binary_cross_entropy_with_logits\n",
        "\n",
        "class MultiLabelTransformer(torch.nn.Module):\n",
        "    \"\"\"PyTorch module used to fine-tune a transformer model. \n",
        "    \n",
        "    The structure of this class is meant to be compatible with the Trainer from the \n",
        "    Transformers library: https://huggingface.co/transformers/main_classes/trainer.html\n",
        "    \"\"\"\n",
        "    def __init__(self, transformer_model, config, n_labels):\n",
        "        super().__init__()\n",
        "        self.transformer_model = transformer_model\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = torch.nn.Linear(config.hidden_size, n_labels)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        \"\"\"During fine-tuning, attention masks, token types ids have to be passed as\n",
        "        argument, with the labels. During inference, the model simply accepts a sequence \n",
        "        of tokens ids and produces a set of binary logit scores.\n",
        "        \"\"\"\n",
        "        pooled_output = self.transformer_model(\n",
        "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "        ).pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits  = self.classifier(pooled_output)\n",
        "        \n",
        "        if labels is None:\n",
        "            return logits\n",
        "        else:\n",
        "            return binary_cross_entropy_with_logits(logits, labels), logits\n",
        "\n",
        "model = MultiLabelTransformer(transformer_model, config, train_labels.shape[1])\n",
        "model.to(device) # Move our model from memory to the GPU's memory\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IqsyJA8pjZby",
        "outputId": "1ba38250-4406-4e83-fccd-2e4f70593122"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_first_step=True,\n",
        "    logging_steps=10000\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Uncomment the lines below if you want to visualize metrics with TensorBoard\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir logs"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='23937' max='23937' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23937/23937 07:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.815400</td>\n",
              "      <td>0.046013</td>\n",
              "      <td>6.290500</td>\n",
              "      <td>5073.549000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.072000</td>\n",
              "      <td>0.043678</td>\n",
              "      <td>6.256000</td>\n",
              "      <td>5101.503000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.042400</td>\n",
              "      <td>0.042802</td>\n",
              "      <td>6.049800</td>\n",
              "      <td>5275.366000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=23937, training_loss=0.05456216362587724, metrics={'train_runtime': 432.4458, 'train_samples_per_second': 55.353, 'total_flos': 0, 'epoch': 3.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s08uRO3jAuV"
      },
      "source": [
        "How fast! Remember that fine-tuning DistilBERT for 3 epochs took close to an hour with the same dataseet and hardware. Here, we obtain similar performances (validation loss around 0.043) in only 7 minutes! Let's check the inference latency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiQhV6B0nxYr"
      },
      "source": [
        "torch.save(model, \"./model.pt\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2UUL2u8kRsE"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWU-c3EokTDg",
        "outputId": "88d677fc-b6b7-40db-d2c0-e948a8665abc"
      },
      "source": [
        "import pandas as pd\n",
        "from time import time\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
        "loaded_model = torch.load(\"./model.pt\")\n",
        "loaded_model.eval()  # Run to finish loading the model\n",
        "\n",
        "def score_text(text, tokenizer=tokenizer, model=loaded_model):\n",
        "    \"\"\"Tokenize and score a single text.\"\"\"\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text, max_length=MAX_LENGTH, padding='max_length', \n",
        "        return_token_type_ids=False, return_attention_mask=False\n",
        "    )\n",
        "\n",
        "    token_ids = torch.tensor(inputs['input_ids'], dtype=torch.long).reshape(1, -1)\n",
        "    token_ids = token_ids.to(device)\n",
        "\n",
        "    logits = model(token_ids)\n",
        "    scores = torch.sigmoid(logits[0])\n",
        "    return scores.cpu().detach().numpy()\n",
        "\n",
        "text = \"\"\"I am a nice Wikipedia user, I mean no harm, \n",
        "I will not insult anybody or be offensive anyhow.\"\"\"\n",
        "\n",
        "t0 = time()\n",
        "scores = score_text(text)\n",
        "latency = time()-t0\n",
        "\n",
        "scores = pd.Series(scores, label_names, name=\"scores\")\n",
        "print(scores.to_frame())\n",
        "print(f\"\\nLatency: {latency:.3f} seconds\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 scores\n",
            "toxic          0.011528\n",
            "severe_toxic   0.000376\n",
            "obscene        0.001974\n",
            "threat         0.000447\n",
            "insult         0.001925\n",
            "identity_hate  0.000574\n",
            "\n",
            "Latency: 0.006 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XH-enUmlGPL"
      },
      "source": [
        "Blazingly fast ⚡️. You can ship your model as an API (using [Cortex](https://www.cortex.dev/), for instance), and nobody should complain about its latency.\n",
        "\n",
        "To sum up, we trained an extremely distilled version of BERT (4.4M parameters instead of 110M) on ~150k comments from Wikipedia in only 7 minutes. We obtain similar performance (validation loss of 0.04) as with DistilBERT for the same number of epochs, i.e. doing way better than a random predictor (validation loss of 0.30) and than a standard ML baseline (validation loss of 0.28). Inference only takes on the order of 5 milliseconds.\n",
        "\n",
        "Those smaller, distilled BERT models are promising for practical applications when needing faster training or inference time, or when using modest hardware. \n",
        "\n",
        "Give them a try next time you want to train a Transformer model! 🤙"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuhO12BtE-mW"
      },
      "source": [
        "## Bonus: fine-tuning with PyTorch Lightning \n",
        "\n",
        "I also experimented with training the model only with PyTorch, while just using the Transformers library to download and extract the pre-trained BERT-Tiny model.\n",
        "\n",
        "I found that, similarly to Keras for TensorFlow, [PyTorch Lightning](https://www.pytorchlightning.ai/) makes your life easier when using PyTorch The developers even provide a [nice template](https://colab.research.google.com/drive/1rHBxrtopwtF8iLpmC_e7yl3TeDGrseJL?usp=sharing%3E#scrollTo=V7ELesz1kVQo) to customize to your needs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8qYhftoFBE-"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning pytorch-lightning-bolts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT5QMVEXFGvY"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics import Accuracy, AveragePrecision\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits\n",
        "\n",
        "class LightningTransformer(pl.LightningModule):\n",
        "    def __init__(self, transformer_model, config, n_labels):\n",
        "        super().__init__()\n",
        "        self.transformer_model = transformer_model\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.n_labels = n_labels\n",
        "        self.classifier = torch.nn.Linear(config.hidden_size, n_labels)\n",
        "    \n",
        "    def forward(self, batch):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        token_type_ids = batch['token_type_ids']\n",
        "\n",
        "        pooled_output = self.transformer_model(\n",
        "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "        ).pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits  = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        labels = batch[\"labels\"]\n",
        "        outputs = self.forward(batch)\n",
        "        loss = binary_cross_entropy_with_logits(outputs, labels)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        labels = batch[\"labels\"]\n",
        "        outputs = self.forward(batch)\n",
        "        loss = binary_cross_entropy_with_logits(outputs, labels)\n",
        "        self.log('val_loss', loss)\n",
        "        \n",
        "        scores = torch.sigmoid(outputs)\n",
        "\n",
        "        preds = scores.round()\n",
        "        matches = torch.tensor(preds == labels, dtype=torch.float)\n",
        "        self.log('binary_accuracy', matches.mean())\n",
        "        self.log('multilabel_accuracy', matches.min(axis=1).values.mean())\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.validation_step(batch, batch_idx)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
        "        return optimizer\n",
        "\n",
        "n_labels = train_labels.shape[1]\n",
        "lightning_transformer = LightningTransformer(transformer_model, config, n_labels)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=3, progress_bar_refresh_rate=20)\n",
        "\n",
        "trainer.fit(lightning_transformer, training_loader, testing_loader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}